{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating a Basic Model with LM Evaluation Harness\n",
        "\n",
        "This notebook demonstrates how to evaluate a basic language model using the LM Evaluation Harness framework.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The LM Evaluation Harness provides a unified framework to test generative language models on various evaluation tasks. In this notebook, we'll:\n",
        "\n",
        "1. Set up the environment and install dependencies\n",
        "2. Load a simple model (HuggingFace transformers)\n",
        "3. Evaluate it on a basic task (HellaSwag)/ set of tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Installation and Setup\n",
        "\n",
        "First, let's ensure we have all the required dependencies installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Obtaining file:///mloscratch/users/hatrouho/lm-evaluation-harness\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (1.8.1)\n",
            "Collecting evaluate (from lm_eval==0.4.9.1)\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (3.6.0)\n",
            "Collecting jsonlines (from lm_eval==0.4.9.1)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting numexpr (from lm_eval==0.4.9.1)\n",
            "  Downloading numexpr-2.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting peft>=0.2.0 (from lm_eval==0.4.9.1)\n",
            "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (2.13.6)\n",
            "Collecting pytablewriter (from lm_eval==0.4.9.1)\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.9.1)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: sacrebleu>=1.5.0 in /home/runai-home/.local/lib/python3.12/site-packages (from lm_eval==0.4.9.1) (2.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (1.6.1)\n",
            "Collecting sqlitedict (from lm_eval==0.4.9.1)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (2.8.0a0+5228986c39.nv25.5)\n",
            "Collecting tqdm-multiprocess (from lm_eval==0.4.9.1)\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (4.53.0)\n",
            "Collecting zstandard (from lm_eval==0.4.9.1)\n",
            "  Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (0.3.8)\n",
            "Collecting word2number (from lm_eval==0.4.9.1)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: more_itertools in /usr/local/lib/python3.12/dist-packages/setuptools/_vendor (from lm_eval==0.4.9.1) (10.3.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (0.33.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (19.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (3.11.16)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (3.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0->lm_eval==0.4.9.1) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0->lm_eval==0.4.9.1) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9.1) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9.1) (2025.4.26)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.9.1) (2.2.2)\n",
            "Collecting nltk (from rouge-score>=0.0.4->lm_eval==0.4.9.1)\n",
            "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.9.1) (1.16.0)\n",
            "Requirement already satisfied: portalocker in /home/runai-home/.local/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9.1) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9.1) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9.1) (0.9.0)\n",
            "Requirement already satisfied: colorama in /home/runai-home/.local/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9.1) (0.4.6)\n",
            "Requirement already satisfied: lxml in /home/runai-home/.local/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9.1) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.9.1) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.9.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.9.1) (3.6.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (78.1.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->lm_eval==0.4.9.1) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1->lm_eval==0.4.9.1) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->lm_eval==0.4.9.1) (3.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.9.1) (8.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.9.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.9.1) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.9.1) (2025.2)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.9.1)\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.9.1)\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.9.1)\n",
            "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.9.1)\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.9.1)\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.9.1)\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.9.1)\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "Downloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading numexpr-2.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (443 kB)\n",
            "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
            "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
            "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: lm_eval, rouge-score, sqlitedict, word2number\n",
            "  Building editable for lm_eval (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for lm_eval: filename=lm_eval-0.4.9.1-0.editable-py3-none-any.whl size=29867 sha256=767919b9766078356d394da342c86487222c822e99f5d1fd43352026e7c3c2d4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qunz_ioc/wheels/fd/3b/e4/4b91c9cbeae61e0565934f781b60ee5e9fe079f73246db9969\n",
            "\u001b[33m  DEPRECATION: Building 'rouge-score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge-score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24988 sha256=f69f7c39ab7481dd86389cabd46ec5c0fcaeb96190d6f3a099a540ed5b75c81a\n",
            "  Stored in directory: /home/runai-home/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "\u001b[33m  DEPRECATION: Building 'sqlitedict' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sqlitedict'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16958 sha256=2e0f4f169c8df5e8ed31e513e192879fea9e573dcb8a9ef1e58faf56b0a8a482\n",
            "  Stored in directory: /home/runai-home/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
            "\u001b[33m  DEPRECATION: Building 'word2number' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'word2number'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5658 sha256=ed814c5ef785351e0329b992bf7804eaaf8a49f4ec0312f5665398eeb39899cd\n",
            "  Stored in directory: /home/runai-home/.cache/pip/wheels/5b/79/fb/d25928e599c7e11fe4e00d32048cd74933f34a74c633d2aea6\n",
            "Successfully built lm_eval rouge-score sqlitedict word2number\n",
            "Installing collected packages: word2number, sqlitedict, zstandard, tqdm-multiprocess, tcolorpy, pathvalidate, numexpr, nltk, jsonlines, chardet, rouge-score, mbstrdecoder, typepy, peft, evaluate, DataProperty, tabledata, pytablewriter, lm_eval\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/19\u001b[0m [lm_eval]5/19\u001b[0m [DataProperty]\n",
            "\u001b[1A\u001b[2KSuccessfully installed DataProperty-1.1.0 chardet-5.2.0 evaluate-0.4.6 jsonlines-4.0.0 lm_eval-0.4.9.1 mbstrdecoder-1.1.4 nltk-3.9.2 numexpr-2.14.1 pathvalidate-3.3.1 peft-0.18.0 pytablewriter-1.2.1 rouge-score-0.1.2 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1 zstandard-0.25.0\n",
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Install the package in development mode if not already installed\n",
        "# Uncomment the line below if you need to install from scratch\n",
        "# !pip install -e .\n",
        "\n",
        "# Check if we can import the package\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the current directory to the path\n",
        "sys.path.insert(0, os.path.abspath('.'))\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0a0+5228986c39.nv25.05\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA H100 80GB HBM3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "from lm_eval import simple_evaluate\n",
        "from lm_eval.tasks import TaskManager\n",
        "from lm_eval.utils import setup_logging\n",
        "\n",
        "# Setup logging to see what's happening\n",
        "setup_logging(verbosity=\"INFO\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Explore Available Tasks\n",
        "\n",
        "Let's see what tasks are available for evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Popular Evaluation Tasks ===\n",
            "✓ hellaswag\n",
            "✓ arc_easy\n",
            "✓ arc_challenge\n",
            "✓ lambada_openai\n",
            "✓ piqa\n",
            "✓ winogrande\n",
            "\n",
            "To see all available tasks, run: lm_eval --tasks list\n"
          ]
        }
      ],
      "source": [
        "# Initialize task manager to explore available tasks\n",
        "task_manager = TaskManager(verbosity=\"INFO\")\n",
        "\n",
        "# List some popular tasks\n",
        "print(\"\\n=== Popular Evaluation Tasks ===\")\n",
        "popular_tasks = [\n",
        "    \"hellaswag\",      # Commonsense reasoning\n",
        "    \"arc_easy\",       # Science questions (easy)\n",
        "    \"arc_challenge\",  # Science questions (challenging)\n",
        "    \"lambada_openai\", # Language modeling\n",
        "    \"piqa\",           # Physical commonsense reasoning\n",
        "    \"winogrande\",     # Pronoun resolution\n",
        "]\n",
        "\n",
        "for task in popular_tasks:\n",
        "    if task in task_manager._all_tasks:\n",
        "        print(f\"✓ {task}\")\n",
        "    else:\n",
        "        print(f\"✗ {task} (not found)\")\n",
        "\n",
        "print(\"\\nTo see all available tasks, run: lm_eval --tasks list\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Choose a Model and Task\n",
        "\n",
        "For this tutorial, we'll use:\n",
        "- **Model**: `EleutherAI/pythia-160m` - A small, fast model perfect for testing\n",
        "- **Task**: `hellaswag` - A commonsense reasoning task\n",
        "\n",
        "You can easily change these to evaluate different models or tasks!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: EleutherAI/pythia-160m\n",
            "Task: hellaswag\n",
            "Device: cuda:0\n",
            "Batch size: 8\n",
            "Limit: 50 examples\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"EleutherAI/pythia-160m\"  # Small model for quick testing\n",
        "# Alternative models to try:\n",
        "#MODEL_NAME = \"gpt2\"  # OpenAI's GPT-2\n",
        "# MODEL_NAME = \"EleutherAI/gpt-neo-125M\"  # GPT-Neo 125M\n",
        "\n",
        "TASK_NAME = \"hellaswag\"  # Commonsense reasoning task\n",
        "# Alternative tasks you caton try:\n",
        "# TASK_NAME = \"arc_easy\"  # Science questions\n",
        "# TASK_NAME = \"piqa\"  # Physical commonsense\n",
        "\n",
        "# Limit the number of examples for faster evaluation (remove or set to None for full evaluation)\n",
        "LIMIT = 50  # Only evaluate on 50 examples for this demo\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 8  # Adjust based on the GPU memory\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Task: {TASK_NAME}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Limit: {LIMIT} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Run Evaluation\n",
        "\n",
        "Now let's evaluate the model using the `simple_evaluate` function. This is the main API for running evaluations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-16:16:14:31 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-11-16:16:14:31 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': 'EleutherAI/pythia-160m', 'dtype': 'float32'}\n",
            "2025-11-16:16:14:31 INFO     [models.huggingface:156] Using device 'cuda:0'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting evaluation of EleutherAI/pythia-160m on hellaswag\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-16:16:14:32 INFO     [models.huggingface:423] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
            "2025-11-16:16:14:46 WARNING  [evaluator:324] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-11-16:16:14:46 INFO     [api.task:434] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 50/50 [00:00<00:00, 4526.16it/s]\n",
            "2025-11-16:16:14:46 INFO     [evaluator:574] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 200/200 [00:00<00:00, 1272.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation complete!\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting evaluation of {MODEL_NAME} on {TASK_NAME}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Run evaluation\n",
        "# The simple_evaluate function is the main entry point\n",
        "results = simple_evaluate(\n",
        "    model=\"hf\",  # Use HuggingFace model backend\n",
        "    model_args=f\"pretrained={MODEL_NAME},dtype=float32\",  # Model arguments\n",
        "    tasks=[TASK_NAME],  # List of tasks to evaluate on\n",
        "    device=DEVICE,  # Device to run on\n",
        "    batch_size=BATCH_SIZE,  # Batch size for evaluation\n",
        "    limit=LIMIT,  # Limit number of examples (for testing)\n",
        "    num_fewshot=0,  # Number of few-shot examples (0 = zero-shot)\n",
        "    log_samples=True,  # Log individual samples for analysis\n",
        "    verbosity=\"INFO\",  # Logging verbosity\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Display Results\n",
        "\n",
        "Let's examine the evaluation results in detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Task: hellaswag\n",
            "------------------------------------------------------------\n",
            "acc,none: 0.3600\n",
            "acc_stderr,none: 0.0686\n",
            "acc_norm,none: 0.4600\n",
            "acc_norm_stderr,none: 0.0712\n",
            "\n",
            "============================================================\n",
            "CONFIGURATION\n",
            "============================================================\n",
            "Model: hf\n",
            "Model args: pretrained=EleutherAI/pythia-160m,dtype=float32\n",
            "Tasks evaluated: ['hellaswag']\n",
            "Batch size: 8\n",
            "Device: cuda:0\n",
            "Limit: 50\n",
            "Number of few-shot examples: 0\n",
            "\n",
            "Samples evaluated: {'original': 10042, 'effective': 50}\n"
          ]
        }
      ],
      "source": [
        "# Display results in a readable format\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The results dictionary from simple_evaluate() contains (as per evaluator.py):\n",
        "# - 'results': Task-specific metrics (dict with task names as keys)\n",
        "# - 'config': Configuration used for evaluation (model, model_args, batch_size, device, etc.)\n",
        "# - 'versions': Version information for each task\n",
        "# - 'samples': Individual sample results (if log_samples=True)\n",
        "# - 'n-shot': Number of few-shot examples per task\n",
        "# - 'higher_is_better': Whether higher metric values are better\n",
        "# - 'n-samples': Number of samples evaluated\n",
        "# - 'configs': Per-task configurations\n",
        "# - 'git_hash': Git commit hash\n",
        "# - 'date': Timestamp of evaluation\n",
        "\n",
        "if results:\n",
        "    # Print task results\n",
        "    if 'results' in results:\n",
        "        task_results = results['results']\n",
        "        if TASK_NAME in task_results:\n",
        "            print(f\"\\nTask: {TASK_NAME}\")\n",
        "            print(\"-\" * 60)\n",
        "            \n",
        "            task_metrics = task_results[TASK_NAME]\n",
        "            for metric_name, metric_value in task_metrics.items():\n",
        "                # Skip internal keys like 'alias' and 'samples'\n",
        "                if metric_name in ['alias', 'samples']:\n",
        "                    continue\n",
        "                if isinstance(metric_value, dict):\n",
        "                    # Handle nested metrics\n",
        "                    print(f\"\\n{metric_name}:\")\n",
        "                    for sub_metric, value in metric_value.items():\n",
        "                        if isinstance(value, (int, float)):\n",
        "                            print(f\"  {sub_metric}: {value:.4f}\")\n",
        "                        else:\n",
        "                            print(f\"  {sub_metric}: {value}\")\n",
        "                elif isinstance(metric_value, (int, float)):\n",
        "                    print(f\"{metric_name}: {metric_value:.4f}\")\n",
        "                else:\n",
        "                    print(f\"{metric_name}: {metric_value}\")\n",
        "    \n",
        "    # Print configuration info\n",
        "    if 'config' in results:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"CONFIGURATION\")\n",
        "        print(\"=\"*60)\n",
        "        config = results['config']\n",
        "        print(f\"Model: {config.get('model', 'N/A')}\")\n",
        "        print(f\"Model args: {config.get('model_args', 'N/A')}\")\n",
        "        print(f\"Tasks evaluated: {list(results.get('results', {}).keys())}\")\n",
        "        print(f\"Batch size: {config.get('batch_size', 'N/A')}\")\n",
        "        print(f\"Device: {config.get('device', 'N/A')}\")\n",
        "        print(f\"Limit: {config.get('limit', 'N/A')}\")\n",
        "        print(f\"Number of few-shot examples: {results.get('n-shot', {}).get(TASK_NAME, 'N/A')}\")\n",
        "    \n",
        "    # Print additional info if available\n",
        "    if 'n-samples' in results and TASK_NAME in results['n-samples']:\n",
        "        print(f\"\\nSamples evaluated: {results['n-samples'][TASK_NAME]}\")\n",
        "else:\n",
        "    print(\"No results returned! (This can happen in multi-GPU setups where rank != 0)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Analyze Individual Samples\n",
        "\n",
        "Let's look at some individual examples to understand how the model is performing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "SAMPLE RESULTS (showing first 3 examples)\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Example 1 ---\n",
            "\n",
            "Prompt/Context:\n",
            "  Context: A man is sitting on a roof. he\n",
            "  Options:\n",
            "    A. is using wrap to wrap a pair of skis.\n",
            "    B. is ripping level tiles off.\n",
            "    C. is holding a rubik's cube.\n",
            "    D. starts pulling up roofing on a roof.\n",
            "\n",
            "Correct answer: D (starts pulling up roofing on a roof.)\n",
            "Model responses: [[(-43.672725677490234, False)], [(-34.15314483642578, False)], [(-28.880355834960938, False)], [(-35.52629852294922, False)]]\n",
            "Filtered responses: [(-43.672725677490234, False), (-34.15314483642578, False), (-28.880355834960938, False), (-35.52629852294922, False)]\n",
            "Result: ✗ INCORRECT\n",
            "\n",
            "--- Example 2 ---\n",
            "\n",
            "Prompt/Context:\n",
            "  Context: A lady walks to a barbell. She bends down and grabs the pole. the lady\n",
            "  Options:\n",
            "    A. swings and lands in her arms.\n",
            "    B. pulls the barbell forward.\n",
            "    C. pulls a rope attached to the barbell.\n",
            "    D. stands and lifts the weight over her head.\n",
            "\n",
            "Correct answer: D (stands and lifts the weight over her head.)\n",
            "Model responses: [[(-25.862987518310547, False)], [(-12.771944046020508, False)], [(-20.074993133544922, False)], [(-26.751115798950195, False)]]\n",
            "Filtered responses: [(-25.862987518310547, False), (-12.771944046020508, False), (-20.074993133544922, False), (-26.751115798950195, False)]\n",
            "Result: ✗ INCORRECT\n",
            "\n",
            "--- Example 3 ---\n",
            "\n",
            "Prompt/Context:\n",
            "  Context: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. the child and a different man\n",
            "  Options:\n",
            "    A. are then shown paddling down a river in a boat while a woman talks.\n",
            "    B. are driving the canoe, they go down the river flowing side to side.\n",
            "    C. sit in a canoe while the man paddles.\n",
            "    D. walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.\n",
            "\n",
            "Correct answer: C (sit in a canoe while the man paddles.)\n",
            "Model responses: [[(-43.15055847167969, False)], [(-52.449371337890625, False)], [(-25.82148551940918, False)], [(-98.88858032226562, False)]]\n",
            "Filtered responses: [(-43.15055847167969, False), (-52.449371337890625, False), (-25.82148551940918, False), (-98.88858032226562, False)]\n",
            "Result: ✓ CORRECT\n"
          ]
        }
      ],
      "source": [
        "# Examine individual samples (if available)\n",
        "if results and 'samples' in results:\n",
        "    samples = results['samples']\n",
        "    \n",
        "    if TASK_NAME in samples and len(samples[TASK_NAME]) > 0:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"SAMPLE RESULTS (showing first 3 examples)\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        \n",
        "        # Show first few samples\n",
        "        for i, sample in enumerate(samples[TASK_NAME][:3]):\n",
        "            print(f\"\\n--- Example {i+1} ---\")\n",
        "            \n",
        "            # Print the input/prompt\n",
        "            if 'doc' in sample:\n",
        "                doc = sample['doc']\n",
        "                print(f\"\\nPrompt/Context:\")\n",
        "                # HellaSwag specific fields\n",
        "                if 'ctx' in doc:\n",
        "                    print(f\"  Context: {doc['ctx']}\")\n",
        "                if 'endings' in doc:\n",
        "                    print(f\"  Options:\")\n",
        "                    for j, ending in enumerate(doc['endings']):\n",
        "                        print(f\"    {chr(65+j)}. {ending}\")\n",
        "            \n",
        "            # Print the correct answer\n",
        "            if 'doc' in sample and 'label' in sample['doc']:\n",
        "                correct_idx = sample['doc']['label']\n",
        "                # Convert to int if it's a string (some tasks store labels as strings)\n",
        "                try:\n",
        "                    correct_idx = int(correct_idx)\n",
        "                except (ValueError, TypeError):\n",
        "                    # If conversion fails, try to find the index in endings\n",
        "                    if 'endings' in sample['doc']:\n",
        "                        # Label might be the actual text, find its index\n",
        "                        try:\n",
        "                            correct_idx = sample['doc']['endings'].index(correct_idx)\n",
        "                        except (ValueError, TypeError):\n",
        "                            print(f\"\\nCorrect answer: {correct_idx} (could not convert to index)\")\n",
        "                            correct_idx = None\n",
        "                \n",
        "                if correct_idx is not None and 'endings' in sample['doc']:\n",
        "                    print(f\"\\nCorrect answer: {chr(65+correct_idx)} ({sample['doc']['endings'][correct_idx]})\")\n",
        "                elif 'endings' in sample['doc']:\n",
        "                    print(f\"\\nCorrect answer: {sample['doc']['label']}\")\n",
        "            \n",
        "            # Print model's prediction\n",
        "            if 'resps' in sample:\n",
        "                print(f\"Model responses: {sample['resps']}\")\n",
        "            \n",
        "            # Print if correct\n",
        "            if 'filtered_resps' in sample:\n",
        "                print(f\"Filtered responses: {sample['filtered_resps']}\")\n",
        "            \n",
        "            # Check if correct\n",
        "            if 'acc' in sample or 'acc_norm' in sample:\n",
        "                acc = sample.get('acc', sample.get('acc_norm', None))\n",
        "                if acc is not None:\n",
        "                    status = \"✓ CORRECT\" if acc == 1.0 else \"✗ INCORRECT\"\n",
        "                    print(f\"Result: {status}\")\n",
        "    else:\n",
        "        print(\"No samples available for this task.\")\n",
        "else:\n",
        "    print(\"Samples not logged. Set log_samples=True to see individual examples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Evaluate on Multiple Tasks\n",
        "\n",
        "We can also evaluate on multiple tasks at once. Let's try a few simple tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-16:16:07:50 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-11-16:16:07:50 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': 'EleutherAI/pythia-160m', 'dtype': 'float32'}\n",
            "2025-11-16:16:07:50 INFO     [models.huggingface:156] Using device 'cuda:0'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Evaluating on multiple tasks: hellaswag, arc_easy, piqa\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-16:16:07:50 INFO     [models.huggingface:423] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
            "Generating train split: 100%|██████████| 2251/2251 [00:00<00:00, 495740.53 examples/s]\n",
            "Generating test split: 100%|██████████| 2376/2376 [00:00<00:00, 664023.61 examples/s]\n",
            "Generating validation split: 100%|██████████| 570/570 [00:00<00:00, 290139.96 examples/s]\n",
            "Generating train split: 100%|██████████| 16113/16113 [00:00<00:00, 1450368.49 examples/s]\n",
            "Generating validation split: 100%|██████████| 1838/1838 [00:00<00:00, 694891.90 examples/s]\n",
            "Generating test split: 100%|██████████| 3084/3084 [00:00<00:00, 1112899.73 examples/s]\n",
            "2025-11-16:16:08:17 WARNING  [evaluator:324] Overwriting default num_fewshot of piqa from None to 0\n",
            "2025-11-16:16:08:17 WARNING  [evaluator:324] Overwriting default num_fewshot of arc_easy from None to 0\n",
            "2025-11-16:16:08:17 WARNING  [evaluator:324] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-11-16:16:08:17 INFO     [api.task:434] Building contexts for piqa on rank 0...\n",
            "100%|██████████| 20/20 [00:00<00:00, 1682.90it/s]\n",
            "2025-11-16:16:08:17 INFO     [api.task:434] Building contexts for arc_easy on rank 0...\n",
            "100%|██████████| 20/20 [00:00<00:00, 1873.46it/s]\n",
            "2025-11-16:16:08:17 INFO     [api.task:434] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 20/20 [00:00<00:00, 4310.69it/s]\n",
            "2025-11-16:16:08:17 INFO     [evaluator:574] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 200/200 [00:00<00:00, 1179.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "MULTI-TASK RESULTS SUMMARY\n",
            "============================================================\n",
            "hellaswag           : {'alias': 'hellaswag', 'acc,none': 0.25, 'acc_stderr,none': 0.09933992677987828, 'acc_norm,none': 0.3, 'acc_norm_stderr,none': 0.10513149660756935}\n",
            "arc_easy            : {'alias': 'arc_easy', 'acc,none': 0.5, 'acc_stderr,none': 0.11470786693528086, 'acc_norm,none': 0.25, 'acc_norm_stderr,none': 0.09933992677987828}\n",
            "piqa                : {'alias': 'piqa', 'acc,none': 0.65, 'acc_stderr,none': 0.1094243309804831, 'acc_norm,none': 0.7, 'acc_norm_stderr,none': 0.10513149660756936}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on multiple tasks\n",
        "MULTIPLE_TASKS = [\"hellaswag\", \"arc_easy\", \"piqa\"]\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Evaluating on multiple tasks: {', '.join(MULTIPLE_TASKS)}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Run evaluation on multiple tasks\n",
        "multi_results = simple_evaluate(\n",
        "    model=\"hf\",\n",
        "    model_args=f\"pretrained={MODEL_NAME},dtype=float32\",\n",
        "    tasks=MULTIPLE_TASKS,\n",
        "    device=DEVICE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    limit=20,  # Small limit for demo\n",
        "    num_fewshot=0,\n",
        "    verbosity=\"INFO\",\n",
        ")\n",
        "\n",
        "# Display summary of results\n",
        "if multi_results and 'results' in multi_results:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MULTI-TASK RESULTS SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for task in MULTIPLE_TASKS:\n",
        "        if task in multi_results['results']:\n",
        "            task_metrics = multi_results['results'][task]\n",
        "            # Extract accuracy metric (common across tasks)\n",
        "            acc = None\n",
        "            if 'acc' in task_metrics:\n",
        "                acc = task_metrics['acc']\n",
        "            elif 'acc_norm' in task_metrics:\n",
        "                acc = task_metrics['acc_norm']\n",
        "            \n",
        "            if acc is not None:\n",
        "                print(f\"{task:20s}: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "            else:\n",
        "                print(f\"{task:20s}: {task_metrics}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Understanding the Results\n",
        "\n",
        "### Key Metrics Explained:\n",
        "\n",
        "1. **Accuracy (acc)**: The percentage of examples the model got correct\n",
        "2. **Normalized Accuracy (acc_norm)**: Accuracy after normalizing the predictions\n",
        "3. **Perplexity**: For language modeling tasks, lower is better\n",
        "\n",
        "### Interpreting HellaSwag Results:\n",
        "\n",
        "- **HellaSwag** is a commonsense reasoning task where the model must choose the best ending for a given context\n",
        "- Random guessing would achieve ~25% accuracy (1 out of 4 choices)\n",
        "- Good models typically achieve 70-90%+ accuracy\n",
        "\n",
        "### Tips for Better Evaluation:\n",
        "\n",
        "1. **Remove the limit**: Set `limit=None` for full evaluation on all test examples\n",
        "2. **Try different models**: Experiment with larger models for better performance\n",
        "3. **Try different tasks**: Each task tests different capabilities\n",
        "4. **Use few-shot learning**: Set `num_fewshot=5` or higher to provide examples\n",
        "5. **Adjust batch size**: Larger batch sizes are faster but require more memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Examples\n",
        "\n",
        "### Example 1: Using a Different Model\n",
        "\n",
        "```python\n",
        "# Evaluate GPT-2\n",
        "results = simple_evaluate(\n",
        "    model=\"hf\",\n",
        "    model_args=\"pretrained=gpt2,dtype=float32\",\n",
        "    tasks=[\"hellaswag\"],\n",
        "    device=\"cuda:0\",\n",
        "    batch_size=8,\n",
        "    limit=100,\n",
        ")\n",
        "```\n",
        "\n",
        "### Example 2: Using Few-Shot Learning\n",
        "\n",
        "```python\n",
        "# Provide 5 examples in the prompt\n",
        "results = simple_evaluate(\n",
        "    model=\"hf\",\n",
        "    model_args=f\"pretrained={MODEL_NAME},dtype=float32\",\n",
        "    tasks=[\"hellaswag\"],\n",
        "    device=DEVICE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_fewshot=5,  # 5-shot learning\n",
        "    limit=100,\n",
        ")\n",
        "```\n",
        "\n",
        "### Example 3: Using the CLI Instead\n",
        "\n",
        "We can also run evaluations from the command line:\n",
        "\n",
        "```bash\n",
        "lm_eval --model hf \\\\\n",
        "    --model_args pretrained=EleutherAI/pythia-160m,dtype=float32 \\\\\n",
        "    --tasks hellaswag \\\\\n",
        "    --device cuda:0 \\\\\n",
        "    --batch_size 8 \\\\\n",
        "    --limit 50\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
