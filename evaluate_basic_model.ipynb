{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating a Basic Model with LM Evaluation Harness\n",
        "\n",
        "This notebook demonstrates how to evaluate a basic language model using the LM Evaluation Harness framework.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The LM Evaluation Harness provides a unified framework to test generative language models on various evaluation tasks. In this notebook, we'll:\n",
        "\n",
        "1. Set up the environment and install dependencies\n",
        "2. Load a simple model (HuggingFace transformers)\n",
        "3. Evaluate it on a basic task (HellaSwag)/ set of tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Installation and Setup\n",
        "\n",
        "First, let's ensure we have all the required dependencies installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Install the package in development mode if not already installed\n",
        "# Uncomment the line below if you need to install from scratch\n",
        "# !pip install -e .\n",
        "\n",
        "# Check if we can import the package\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the current directory to the path\n",
        "sys.path.insert(0, os.path.abspath('.'))\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0a0+5228986c39.nv25.05\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA H100 80GB HBM3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "from lm_eval import simple_evaluate\n",
        "from lm_eval.tasks import TaskManager\n",
        "from lm_eval.utils import setup_logging\n",
        "\n",
        "# Setup logging to see what's happening\n",
        "setup_logging(verbosity=\"INFO\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Explore Available Tasks\n",
        "\n",
        "Let's see what tasks are available for evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Popular Evaluation Tasks ===\n",
            "✓ hellaswag\n",
            "✓ arc_easy\n",
            "✓ arc_challenge\n",
            "✓ lambada_openai\n",
            "✓ piqa\n",
            "✓ winogrande\n",
            "\n",
            "To see all available tasks, run: lm_eval --tasks list\n"
          ]
        }
      ],
      "source": [
        "# Initialize task manager to explore available tasks\n",
        "task_manager = TaskManager(verbosity=\"INFO\")\n",
        "\n",
        "# List some popular tasks\n",
        "print(\"\\n=== Popular Evaluation Tasks ===\")\n",
        "popular_tasks = [\n",
        "    \"hellaswag\",      # Commonsense reasoning\n",
        "    \"arc_easy\",       # Science questions (easy)\n",
        "    \"arc_challenge\",  # Science questions (challenging)\n",
        "    \"lambada_openai\", # Language modeling\n",
        "    \"piqa\",           # Physical commonsense reasoning\n",
        "    \"winogrande\",     # Pronoun resolution\n",
        "]\n",
        "\n",
        "for task in popular_tasks:\n",
        "    if task in task_manager._all_tasks:\n",
        "        print(f\"✓ {task}\")\n",
        "    else:\n",
        "        print(f\"✗ {task} (not found)\")\n",
        "\n",
        "print(\"\\nTo see all available tasks, run: lm_eval --tasks list\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Choose a Model and Task\n",
        "\n",
        "For this tutorial, we'll use:\n",
        "- **Model**: `EleutherAI/pythia-160m` - A small, fast model perfect for testing\n",
        "- **Task**: `hellaswag` - A commonsense reasoning task\n",
        "\n",
        "You can easily change these to evaluate different models or tasks!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: EleutherAI/pythia-160m\n",
            "Task: hellaswag\n",
            "Device: cuda:0\n",
            "Batch size: 8\n",
            "Limit: 50 examples\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"EleutherAI/pythia-160m\"  # Small model for quick testing\n",
        "# Alternative models to try:\n",
        "#MODEL_NAME = \"gpt2\"  # OpenAI's GPT-2\n",
        "# MODEL_NAME = \"EleutherAI/gpt-neo-125M\"  # GPT-Neo 125M\n",
        "\n",
        "TASK_NAME = \"hellaswag\"  # Commonsense reasoning task\n",
        "# Alternative tasks you caton try:\n",
        "# TASK_NAME = \"arc_easy\"  # Science questions\n",
        "# TASK_NAME = \"piqa\"  # Physical commonsense\n",
        "\n",
        "# Limit the number of examples for faster evaluation (remove or set to None for full evaluation)\n",
        "LIMIT = 50  # Only evaluate on 50 examples for this demo\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 8  # Adjust based on the GPU memory\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Task: {TASK_NAME}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Limit: {LIMIT} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Run Evaluation\n",
        "\n",
        "Now let's evaluate the model using the `simple_evaluate` function. This is the main API for running evaluations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Direct Model Testing (Alternative)\n",
        "\n",
        "For easier debugging, you can also test the model class directly without going through the full evaluation harness. This is useful for:\n",
        "- Testing schema loading\n",
        "- Testing JSON extraction\n",
        "- Testing validation\n",
        "- Debugging specific issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Test 1: Model Instantiation\n",
            "============================================================\n",
            "✗ Model instantiation failed: attempted to use 'sglang' LM type, but package `sglang` is not installed. Please install sglang via official document here:https://docs.sglang.ai/start/install.html#install-sglang\n",
            "\n",
            "============================================================\n",
            "Direct testing complete!\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_8101/1124215433.py\", line 85, in <module>\n",
            "    test_model = SGLangSchemaLM.create_from_arg_string(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/mloscratch/users/hatrouho/lm-evaluation-harness/lm_eval/models/sglang_schema.py\", line 951, in create_from_arg_string\n",
            "    return cls(**args)\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/mloscratch/users/hatrouho/lm-evaluation-harness/lm_eval/models/sglang_schema.py\", line 93, in __init__\n",
            "    super().__init__(pretrained=pretrained, **kwargs)\n",
            "  File \"/mloscratch/users/hatrouho/lm-evaluation-harness/lm_eval/models/sglang_causallms.py\", line 70, in __init__\n",
            "    raise ModuleNotFoundError(\n",
            "ModuleNotFoundError: attempted to use 'sglang' LM type, but package `sglang` is not installed. Please install sglang via official document here:https://docs.sglang.ai/start/install.html#install-sglang\n"
          ]
        }
      ],
      "source": [
        "# Direct testing of the SGLangSchemaLM class\n",
        "# This is useful for debugging individual components\n",
        "\n",
        "from lm_eval.models.sglang_schema import SGLangSchemaLM\n",
        "from lm_eval.api.instance import Instance\n",
        "\n",
        "# Define configuration (can be run independently)\n",
        "#SCHEMA_MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"  # Use a model compatible with SGLang\n",
        "# Alternative: Use your medical model path\n",
        "SCHEMA_MODEL_NAME = \"OpenMeditron/Meditron3-8B\"\n",
        "\n",
        "schema_file_path = \"test_schema.json\"  # Should exist from previous cell, or create it here\n",
        "\n",
        "# Create schema file if it doesn't exist\n",
        "import json\n",
        "import os\n",
        "if not os.path.exists(schema_file_path):\n",
        "    test_schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"answer\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The answer to the question\"\n",
        "            },\n",
        "            \"confidence\": {\n",
        "                \"type\": \"number\",\n",
        "                \"minimum\": 0,\n",
        "                \"maximum\": 1,\n",
        "                \"description\": \"Confidence score between 0 and 1\"\n",
        "            },\n",
        "            \"reasoning\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Brief reasoning for the answer\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"answer\", \"confidence\"]\n",
        "    }\n",
        "    with open(schema_file_path, 'w') as f:\n",
        "        json.dump(test_schema, f, indent=2)\n",
        "    print(f\"Created schema file: {schema_file_path}\")\n",
        "\n",
        "# Test 1: Check if the model can be instantiated\n",
        "print(\"=\"*60)\n",
        "print(\"Test 1: Model Instantiation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    # Create model instance with schema\n",
        "    test_model = SGLangSchemaLM.create_from_arg_string(\n",
        "        f\"pretrained={SCHEMA_MODEL_NAME},\"\n",
        "        f\"response_schema={schema_file_path},\"\n",
        "        \"validate_with_pydantic=True\"\n",
        "    )\n",
        "    print(\"✓ Model instantiated successfully\")\n",
        "    print(f\"  Schema loaded: {test_model.response_schema is not None}\")\n",
        "    print(f\"  Pydantic model created: {test_model.schema_model is not None}\")\n",
        "    if test_model.schema_model:\n",
        "        print(f\"  Pydantic model name: {test_model.schema_model.__name__}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Model instantiation failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    test_model = None\n",
        "\n",
        "# Test 2: Test JSON extraction (if model was created)\n",
        "if test_model:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Test 2: JSON Extraction\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    test_cases = [\n",
        "        '{\"answer\": \"test\", \"confidence\": 0.9}',\n",
        "        '```json\\n{\"answer\": \"test\", \"confidence\": 0.9}\\n```',\n",
        "        'Some text {\"answer\": \"test\", \"confidence\": 0.9} more text',\n",
        "    ]\n",
        "    \n",
        "    for i, test_text in enumerate(test_cases, 1):\n",
        "        extracted = test_model._extract_json(test_text)\n",
        "        print(f\"  Test {i}: {extracted is not None}\")\n",
        "        if extracted:\n",
        "            print(f\"    Extracted: {extracted[:50]}...\")\n",
        "\n",
        "# Test 3: Test schema validation (if model was created)\n",
        "if test_model and test_model.schema_model:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Test 3: Schema Validation\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Valid JSON matching schema\n",
        "    valid_json = '{\"answer\": \"This is a test answer\", \"confidence\": 0.85}'\n",
        "    is_valid, model_instance, error = test_model._validate_schema(valid_json)\n",
        "    print(f\"  Valid JSON: {is_valid}\")\n",
        "    if is_valid:\n",
        "        print(f\"    ✓ Validation passed\")\n",
        "        print(f\"    Model instance: {model_instance}\")\n",
        "    else:\n",
        "        print(f\"    ✗ Validation failed: {error}\")\n",
        "    \n",
        "    # Invalid JSON (missing required field)\n",
        "    invalid_json = '{\"answer\": \"test\"}'  # Missing \"confidence\"\n",
        "    is_valid, model_instance, error = test_model._validate_schema(invalid_json)\n",
        "    print(f\"\\n  Invalid JSON (missing field): {is_valid}\")\n",
        "    if not is_valid:\n",
        "        print(f\"    ✓ Correctly rejected: {error[:100]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Direct testing complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-20:14:48:21 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-11-20:14:48:21 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': 'EleutherAI/pythia-160m', 'dtype': 'float32'}\n",
            "2025-11-20:14:48:21 INFO     [models.huggingface:156] Using device 'cuda:0'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting evaluation of EleutherAI/pythia-160m on hellaswag\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-20:14:48:22 INFO     [models.huggingface:423] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
            "2025-11-20:14:48:40 WARNING  [evaluator:324] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-11-20:14:48:40 INFO     [api.task:434] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 50/50 [00:00<00:00, 4462.79it/s]\n",
            "2025-11-20:14:48:40 INFO     [evaluator:574] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 200/200 [00:00<00:00, 476.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation complete!\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting evaluation of {MODEL_NAME} on {TASK_NAME}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Run evaluation\n",
        "# The simple_evaluate function is the main entry point\n",
        "results = simple_evaluate(\n",
        "    model=\"hf\",  # Use HuggingFace model backend\n",
        "    model_args=f\"pretrained={MODEL_NAME},dtype=float32\",  # Model arguments\n",
        "    tasks=[TASK_NAME],  # List of tasks to evaluate on\n",
        "    device=DEVICE,  # Device to run on\n",
        "    batch_size=BATCH_SIZE,  # Batch size for evaluation\n",
        "    limit=LIMIT,  # Limit number of examples (for testing)\n",
        "    num_fewshot=0,  # Number of few-shot examples (0 = zero-shot)\n",
        "    log_samples=True,  # Log individual samples for analysis\n",
        "    verbosity=\"INFO\",  # Logging verbosity\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Display Results\n",
        "\n",
        "Let's examine the evaluation results in detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Task: hellaswag\n",
            "------------------------------------------------------------\n",
            "acc,none: 0.3600\n",
            "acc_stderr,none: 0.0686\n",
            "acc_norm,none: 0.4600\n",
            "acc_norm_stderr,none: 0.0712\n",
            "\n",
            "============================================================\n",
            "CONFIGURATION\n",
            "============================================================\n",
            "Model: hf\n",
            "Model args: pretrained=EleutherAI/pythia-160m,dtype=float32\n",
            "Tasks evaluated: ['hellaswag']\n",
            "Batch size: 8\n",
            "Device: cuda:0\n",
            "Limit: 50\n",
            "Number of few-shot examples: 0\n",
            "\n",
            "Samples evaluated: {'original': 10042, 'effective': 50}\n"
          ]
        }
      ],
      "source": [
        "# Display results in a readable format\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The results dictionary from simple_evaluate() contains (as per evaluator.py):\n",
        "# - 'results': Task-specific metrics (dict with task names as keys)\n",
        "# - 'config': Configuration used for evaluation (model, model_args, batch_size, device, etc.)\n",
        "# - 'versions': Version information for each task\n",
        "# - 'samples': Individual sample results (if log_samples=True)\n",
        "# - 'n-shot': Number of few-shot examples per task\n",
        "# - 'higher_is_better': Whether higher metric values are better\n",
        "# - 'n-samples': Number of samples evaluated\n",
        "# - 'configs': Per-task configurations\n",
        "# - 'git_hash': Git commit hash\n",
        "# - 'date': Timestamp of evaluation\n",
        "\n",
        "if results:\n",
        "    # Print task results\n",
        "    if 'results' in results:\n",
        "        task_results = results['results']\n",
        "        if TASK_NAME in task_results:\n",
        "            print(f\"\\nTask: {TASK_NAME}\")\n",
        "            print(\"-\" * 60)\n",
        "            \n",
        "            task_metrics = task_results[TASK_NAME]\n",
        "            for metric_name, metric_value in task_metrics.items():\n",
        "                # Skip internal keys like 'alias' and 'samples'\n",
        "                if metric_name in ['alias', 'samples']:\n",
        "                    continue\n",
        "                if isinstance(metric_value, dict):\n",
        "                    # Handle nested metrics\n",
        "                    print(f\"\\n{metric_name}:\")\n",
        "                    for sub_metric, value in metric_value.items():\n",
        "                        if isinstance(value, (int, float)):\n",
        "                            print(f\"  {sub_metric}: {value:.4f}\")\n",
        "                        else:\n",
        "                            print(f\"  {sub_metric}: {value}\")\n",
        "                elif isinstance(metric_value, (int, float)):\n",
        "                    print(f\"{metric_name}: {metric_value:.4f}\")\n",
        "                else:\n",
        "                    print(f\"{metric_name}: {metric_value}\")\n",
        "    \n",
        "    # Print configuration info\n",
        "    if 'config' in results:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"CONFIGURATION\")\n",
        "        print(\"=\"*60)\n",
        "        config = results['config']\n",
        "        print(f\"Model: {config.get('model', 'N/A')}\")\n",
        "        print(f\"Model args: {config.get('model_args', 'N/A')}\")\n",
        "        print(f\"Tasks evaluated: {list(results.get('results', {}).keys())}\")\n",
        "        print(f\"Batch size: {config.get('batch_size', 'N/A')}\")\n",
        "        print(f\"Device: {config.get('device', 'N/A')}\")\n",
        "        print(f\"Limit: {config.get('limit', 'N/A')}\")\n",
        "        print(f\"Number of few-shot examples: {results.get('n-shot', {}).get(TASK_NAME, 'N/A')}\")\n",
        "    \n",
        "    # Print additional info if available\n",
        "    if 'n-samples' in results and TASK_NAME in results['n-samples']:\n",
        "        print(f\"\\nSamples evaluated: {results['n-samples'][TASK_NAME]}\")\n",
        "else:\n",
        "    print(\"No results returned! (This can happen in multi-GPU setups where rank != 0)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Analyze Individual Samples\n",
        "\n",
        "Let's look at some individual examples to understand how the model is performing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "SAMPLE RESULTS (showing first 3 examples)\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Example 1 ---\n",
            "\n",
            "Prompt/Context:\n",
            "  Context: A man is sitting on a roof. he\n",
            "  Options:\n",
            "    A. is using wrap to wrap a pair of skis.\n",
            "    B. is ripping level tiles off.\n",
            "    C. is holding a rubik's cube.\n",
            "    D. starts pulling up roofing on a roof.\n",
            "\n",
            "Correct answer: D (starts pulling up roofing on a roof.)\n",
            "Model responses: [[(-43.672725677490234, False)], [(-34.15314483642578, False)], [(-28.880355834960938, False)], [(-35.52629852294922, False)]]\n",
            "Filtered responses: [(-43.672725677490234, False), (-34.15314483642578, False), (-28.880355834960938, False), (-35.52629852294922, False)]\n",
            "Result: ✗ INCORRECT\n",
            "\n",
            "--- Example 2 ---\n",
            "\n",
            "Prompt/Context:\n",
            "  Context: A lady walks to a barbell. She bends down and grabs the pole. the lady\n",
            "  Options:\n",
            "    A. swings and lands in her arms.\n",
            "    B. pulls the barbell forward.\n",
            "    C. pulls a rope attached to the barbell.\n",
            "    D. stands and lifts the weight over her head.\n",
            "\n",
            "Correct answer: D (stands and lifts the weight over her head.)\n",
            "Model responses: [[(-25.862987518310547, False)], [(-12.771944046020508, False)], [(-20.074993133544922, False)], [(-26.751115798950195, False)]]\n",
            "Filtered responses: [(-25.862987518310547, False), (-12.771944046020508, False), (-20.074993133544922, False), (-26.751115798950195, False)]\n",
            "Result: ✗ INCORRECT\n",
            "\n",
            "--- Example 3 ---\n",
            "\n",
            "Prompt/Context:\n",
            "  Context: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. the child and a different man\n",
            "  Options:\n",
            "    A. are then shown paddling down a river in a boat while a woman talks.\n",
            "    B. are driving the canoe, they go down the river flowing side to side.\n",
            "    C. sit in a canoe while the man paddles.\n",
            "    D. walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.\n",
            "\n",
            "Correct answer: C (sit in a canoe while the man paddles.)\n",
            "Model responses: [[(-43.15055847167969, False)], [(-52.449371337890625, False)], [(-25.82148551940918, False)], [(-98.88858032226562, False)]]\n",
            "Filtered responses: [(-43.15055847167969, False), (-52.449371337890625, False), (-25.82148551940918, False), (-98.88858032226562, False)]\n",
            "Result: ✓ CORRECT\n"
          ]
        }
      ],
      "source": [
        "# Examine individual samples (if available)\n",
        "if results and 'samples' in results:\n",
        "    samples = results['samples']\n",
        "    \n",
        "    if TASK_NAME in samples and len(samples[TASK_NAME]) > 0:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"SAMPLE RESULTS (showing first 3 examples)\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        \n",
        "        # Show first few samples\n",
        "        for i, sample in enumerate(samples[TASK_NAME][:3]):\n",
        "            print(f\"\\n--- Example {i+1} ---\")\n",
        "            \n",
        "            # Print the input/prompt\n",
        "            if 'doc' in sample:\n",
        "                doc = sample['doc']\n",
        "                print(f\"\\nPrompt/Context:\")\n",
        "                # HellaSwag specific fields\n",
        "                if 'ctx' in doc:\n",
        "                    print(f\"  Context: {doc['ctx']}\")\n",
        "                if 'endings' in doc:\n",
        "                    print(f\"  Options:\")\n",
        "                    for j, ending in enumerate(doc['endings']):\n",
        "                        print(f\"    {chr(65+j)}. {ending}\")\n",
        "            \n",
        "            # Print the correct answer\n",
        "            if 'doc' in sample and 'label' in sample['doc']:\n",
        "                correct_idx = sample['doc']['label']\n",
        "                # Convert to int if it's a string (some tasks store labels as strings)\n",
        "                try:\n",
        "                    correct_idx = int(correct_idx)\n",
        "                except (ValueError, TypeError):\n",
        "                    # If conversion fails, try to find the index in endings\n",
        "                    if 'endings' in sample['doc']:\n",
        "                        # Label might be the actual text, find its index\n",
        "                        try:\n",
        "                            correct_idx = sample['doc']['endings'].index(correct_idx)\n",
        "                        except (ValueError, TypeError):\n",
        "                            print(f\"\\nCorrect answer: {correct_idx} (could not convert to index)\")\n",
        "                            correct_idx = None\n",
        "                \n",
        "                if correct_idx is not None and 'endings' in sample['doc']:\n",
        "                    print(f\"\\nCorrect answer: {chr(65+correct_idx)} ({sample['doc']['endings'][correct_idx]})\")\n",
        "                elif 'endings' in sample['doc']:\n",
        "                    print(f\"\\nCorrect answer: {sample['doc']['label']}\")\n",
        "            \n",
        "            # Print model's prediction\n",
        "            if 'resps' in sample:\n",
        "                print(f\"Model responses: {sample['resps']}\")\n",
        "            \n",
        "            # Print if correct\n",
        "            if 'filtered_resps' in sample:\n",
        "                print(f\"Filtered responses: {sample['filtered_resps']}\")\n",
        "            \n",
        "            # Check if correct\n",
        "            if 'acc' in sample or 'acc_norm' in sample:\n",
        "                acc = sample.get('acc', sample.get('acc_norm', None))\n",
        "                if acc is not None:\n",
        "                    status = \"✓ CORRECT\" if acc == 1.0 else \"✗ INCORRECT\"\n",
        "                    print(f\"Result: {status}\")\n",
        "    else:\n",
        "        print(\"No samples available for this task.\")\n",
        "else:\n",
        "    print(\"Samples not logged. Set log_samples=True to see individual examples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Evaluate on Multiple Tasks\n",
        "\n",
        "We can also evaluate on multiple tasks at once. Let's try a few simple tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-16:16:07:50 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-11-16:16:07:50 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': 'EleutherAI/pythia-160m', 'dtype': 'float32'}\n",
            "2025-11-16:16:07:50 INFO     [models.huggingface:156] Using device 'cuda:0'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Evaluating on multiple tasks: hellaswag, arc_easy, piqa\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-16:16:07:50 INFO     [models.huggingface:423] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
            "Generating train split: 100%|██████████| 2251/2251 [00:00<00:00, 495740.53 examples/s]\n",
            "Generating test split: 100%|██████████| 2376/2376 [00:00<00:00, 664023.61 examples/s]\n",
            "Generating validation split: 100%|██████████| 570/570 [00:00<00:00, 290139.96 examples/s]\n",
            "Generating train split: 100%|██████████| 16113/16113 [00:00<00:00, 1450368.49 examples/s]\n",
            "Generating validation split: 100%|██████████| 1838/1838 [00:00<00:00, 694891.90 examples/s]\n",
            "Generating test split: 100%|██████████| 3084/3084 [00:00<00:00, 1112899.73 examples/s]\n",
            "2025-11-16:16:08:17 WARNING  [evaluator:324] Overwriting default num_fewshot of piqa from None to 0\n",
            "2025-11-16:16:08:17 WARNING  [evaluator:324] Overwriting default num_fewshot of arc_easy from None to 0\n",
            "2025-11-16:16:08:17 WARNING  [evaluator:324] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2025-11-16:16:08:17 INFO     [api.task:434] Building contexts for piqa on rank 0...\n",
            "100%|██████████| 20/20 [00:00<00:00, 1682.90it/s]\n",
            "2025-11-16:16:08:17 INFO     [api.task:434] Building contexts for arc_easy on rank 0...\n",
            "100%|██████████| 20/20 [00:00<00:00, 1873.46it/s]\n",
            "2025-11-16:16:08:17 INFO     [api.task:434] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 20/20 [00:00<00:00, 4310.69it/s]\n",
            "2025-11-16:16:08:17 INFO     [evaluator:574] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|██████████| 200/200 [00:00<00:00, 1179.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "MULTI-TASK RESULTS SUMMARY\n",
            "============================================================\n",
            "hellaswag           : {'alias': 'hellaswag', 'acc,none': 0.25, 'acc_stderr,none': 0.09933992677987828, 'acc_norm,none': 0.3, 'acc_norm_stderr,none': 0.10513149660756935}\n",
            "arc_easy            : {'alias': 'arc_easy', 'acc,none': 0.5, 'acc_stderr,none': 0.11470786693528086, 'acc_norm,none': 0.25, 'acc_norm_stderr,none': 0.09933992677987828}\n",
            "piqa                : {'alias': 'piqa', 'acc,none': 0.65, 'acc_stderr,none': 0.1094243309804831, 'acc_norm,none': 0.7, 'acc_norm_stderr,none': 0.10513149660756936}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on multiple tasks\n",
        "MULTIPLE_TASKS = [\"hellaswag\", \"arc_easy\", \"piqa\"]\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Evaluating on multiple tasks: {', '.join(MULTIPLE_TASKS)}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Run evaluation on multiple tasks\n",
        "multi_results = simple_evaluate(\n",
        "    model=\"hf\",\n",
        "    model_args=f\"pretrained={MODEL_NAME},dtype=float32\",\n",
        "    tasks=MULTIPLE_TASKS,\n",
        "    device=DEVICE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    limit=20,  # Small limit for demo\n",
        "    num_fewshot=0,\n",
        "    verbosity=\"INFO\",\n",
        ")\n",
        "\n",
        "# Display summary of results\n",
        "if multi_results and 'results' in multi_results:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MULTI-TASK RESULTS SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for task in MULTIPLE_TASKS:\n",
        "        if task in multi_results['results']:\n",
        "            task_metrics = multi_results['results'][task]\n",
        "            # Extract accuracy metric (common across tasks)\n",
        "            acc = None\n",
        "            if 'acc' in task_metrics:\n",
        "                acc = task_metrics['acc']\n",
        "            elif 'acc_norm' in task_metrics:\n",
        "                acc = task_metrics['acc_norm']\n",
        "            \n",
        "            if acc is not None:\n",
        "                print(f\"{task:20s}: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "            else:\n",
        "                print(f\"{task:20s}: {task_metrics}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Understanding the Results\n",
        "\n",
        "### Key Metrics Explained:\n",
        "\n",
        "1. **Accuracy (acc)**: The percentage of examples the model got correct\n",
        "2. **Normalized Accuracy (acc_norm)**: Accuracy after normalizing the predictions\n",
        "3. **Perplexity**: For language modeling tasks, lower is better\n",
        "\n",
        "### Interpreting HellaSwag Results:\n",
        "\n",
        "- **HellaSwag** is a commonsense reasoning task where the model must choose the best ending for a given context\n",
        "- Random guessing would achieve ~25% accuracy (1 out of 4 choices)\n",
        "- Good models typically achieve 70-90%+ accuracy\n",
        "\n",
        "### Tips for Better Evaluation:\n",
        "\n",
        "1. **Remove the limit**: Set `limit=None` for full evaluation on all test examples\n",
        "2. **Try different models**: Experiment with larger models for better performance\n",
        "3. **Try different tasks**: Each task tests different capabilities\n",
        "4. **Use few-shot learning**: Set `num_fewshot=5` or higher to provide examples\n",
        "5. **Adjust batch size**: Larger batch sizes are faster but require more memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Examples\n",
        "\n",
        "### Example 1: Using a Different Model\n",
        "\n",
        "```python\n",
        "# Evaluate GPT-2\n",
        "results = simple_evaluate(\n",
        "    model=\"hf\",\n",
        "    model_args=\"pretrained=gpt2,dtype=float32\",\n",
        "    tasks=[\"hellaswag\"],\n",
        "    device=\"cuda:0\",\n",
        "    batch_size=8,\n",
        "    limit=100,\n",
        ")\n",
        "```\n",
        "\n",
        "### Example 2: Using Few-Shot Learning\n",
        "\n",
        "```python\n",
        "# Provide 5 examples in the prompt\n",
        "results = simple_evaluate(\n",
        "    model=\"hf\",\n",
        "    model_args=f\"pretrained={MODEL_NAME},dtype=float32\",\n",
        "    tasks=[\"hellaswag\"],\n",
        "    device=DEVICE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_fewshot=5,  # 5-shot learning\n",
        "    limit=100,\n",
        ")\n",
        "```\n",
        "\n",
        "### Example 3: Using the CLI Instead\n",
        "\n",
        "We can also run evaluations from the command line:\n",
        "\n",
        "```bash\n",
        "lm_eval --model hf \\\\\n",
        "    --model_args pretrained=EleutherAI/pythia-160m,dtype=float32 \\\\\n",
        "    --tasks hellaswag \\\\\n",
        "    --device cuda:0 \\\\\n",
        "    --batch_size 8 \\\\\n",
        "    --limit 50\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
